{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PF4J_U0liQko"
   },
   "source": [
    "## Tensorflow Tutorial 2: Build First Deep Neurel Network (DNN)\n",
    "\n",
    "接續著[上一回](http://www.ycc.idv.tw/YCNote/post/38)，我們已經有一個單層的Neurel Network，緊接著我們來試著一步一步改造它，讓它成為我們常使用的Deep Neurel Network的形式。\n",
    "\n",
    "本單元程式碼可於[Github](https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py)下載。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e51Y_c-iiQkq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6I2JZ8BiQku"
   },
   "source": [
    "### 增加Hidden Layer\n",
    "\n",
    "在上一回當中，我們只有一層Neurel Network，也就是做完一個線性轉換後，就直接使用Softmax Layer來轉換成機率表示方式，這樣的結構並不夠powerful，我們需要把它的結構弄的又窄又深，這樣效果才會好，詳細原因請參考[這一篇的介紹](http://www.ycc.idv.tw/YCNote/post/35)。\n",
    "\n",
    "因此，我們來試著加入一層Hidden Layer，來打造成兩層的Neurel Network，並在兩層之間加入Activation Function，為我的Model增加非線性因子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2dKV3TviQkv"
   },
   "outputs": [],
   "source": [
    "def structure(self, features, labels, n_hidden, activation, dropout_ratio=0, train=False):\n",
    "    # build neurel network structure and return their predictions and loss\n",
    "    ### Variable\n",
    "    if (not self.weights) or (not self.biases):\n",
    "        self.weights = {\n",
    "            'fc1': tf.Variable(tf.truncated_normal(shape=(self.n_features, n_hidden))),\n",
    "            'fc2': tf.Variable(tf.truncated_normal(shape=(n_hidden, self.n_labels))),\n",
    "        }\n",
    "        self.biases = {\n",
    "            'fc1': tf.Variable(tf.zeros(shape=(n_hidden))),\n",
    "            'fc2': tf.Variable(tf.zeros(shape=(self.n_labels))),\n",
    "        }\n",
    "    ### Structure\n",
    "    # layer 1\n",
    "    fc1 = self.getDenseLayer(features, self.weights['fc1'],\n",
    "                                       self.biases['fc1'], activation=activation)\n",
    "    if train:\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob=1-dropout_ratio)\n",
    "\n",
    "    # layer 2\n",
    "    logits = self.getDenseLayer(fc1, self.weights['fc2'], self.biases['fc2'])\n",
    "\n",
    "    y_ = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    return (y_, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3Y-K2oiiQky"
   },
   "source": [
    "首先在變數需要有二層的fully-connect參數，注意這些參數的大小，受到Hidden Layer的神經元數目`n_hidden`決定。接下來開始建構整個Neurel Network的結構，`fc1`產生一個fully-connect的結果，並且通過Activation Function再輸出，然後進到下一層，第二層直接使用`fc1`的結果當作新的輸入，再做一次fully-connect，並且讓它通過Softmax Layer來完成最後的Logistic轉換，它的loss一樣的是使用cross-entropy來評估。\n",
    "\n",
    "### Activation Function的選擇\n",
    "\n",
    "剛剛提到的Hidden Layer可以採用不同的Activation Function，我列幾個常使用的Activation Function給大家看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "Yz_EVDTmiQkz",
    "outputId": "5a23d9a7-91a5-492e-ea57-ef5694ae2be4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8FVX+//HXJ8kloYQAoZckIEWa\nBAhIFQREBAREUVkVERXL2r+rsK6LXbH87AVZcC0roIgiq6AUKRFB2oIiTVBKkBogIZCE3Nzz++ME\nkkBouTeZWz7Px2MeMzd3cuczEd+ZnDlzjhhjUEopFVrCnC5AKaVU6dPwV0qpEKThr5RSIUjDXyml\nQpCGv1JKhSANf6WUCkEa/kopFYI0/JVSKgRp+CulVAiKcLqA06latapJSEhwugyllAooK1eu3G+M\nqXa2/fw2/BMSElixYoXTZSilVEARkW3nsp82+yilVAjS8FdKqRCk4a+UUiHIb9v8i5KTk0NKSgpZ\nWVlOl+L3oqKiqFu3Li6Xy+lSlFJ+KKDCPyUlhejoaBISEhARp8vxW8YYUlNTSUlJoX79+k6Xo5Ty\nQ143+4hIlIgsE5E1IvKriDxZxD6RIvKpiGwWkZ9EJKE4x8rKyiI2NlaD/yxEhNjYWP0LSSl1Wr5o\n888GehhjWgGJQB8R6XDSPrcCB40xDYFXgReKezAN/nOjPyel1Jl4Hf7Gysh76cpbTp4bciDwYd72\n50BP0XRSSqlTffUVfPxxiR/GJ719RCRcRFYDe4E5xpifTtqlDrADwBjjBtKA2CI+Z6SIrBCRFfv2\n7fNFaT536NAh3nnnnWJ/f/fu3fXhNaVU0T7/HK65BsaNg9zcEj2UT8LfGJNrjEkE6gLtRaRFMT9n\nvDEmyRiTVK3aWZ9OdoS34a+UUkWaNAmuvx4uvhhmzYLw8BI9nE/7+RtjDgHzgT4nvbUTqAcgIhFA\nDJDqy2OXltGjR7NlyxYSExN58MEH6dmzJ23atKFly5Z89dVXAGzdupWmTZty++2307x5c3r37k1m\nZuaJz5g6dSrt27encePGJCcnO3UqSil/8eGHcNNN0KULfPstVKxY4of0uquniFQDcowxh0SkLHAZ\np97QnQHcDCwBrgG+N8acfF/gvDzw7QOs3r3am484RWLNRF7r89oZ9xk7dixr165l9erVuN1ujh49\nSsWKFdm/fz8dOnRgwIABAPz2229MnjyZf/3rX1x77bVMmzaNG2+8EQC3282yZcuYOXMmTz75JHPn\nzvXpeSilAsiECTByJPTsadv7y5UrlcP6op9/LeBDEQnH/iXxmTHmaxF5ClhhjJkBTAQ+FpHNwAHg\neh8c13HGGB599FEWLVpEWFgYO3fuZM+ePQDUr1+fxMREANq2bcvWrVtPfN/gwYOL/LpSKsS88w78\n9a9wxRXwxRcQFVVqh/Y6/I0xPwOti/j6mALbWcAQb49V0Nmu0EvDJ598wr59+1i5ciUul4uEhIQT\nfesjIyNP7BceHl6o2ef4e+Hh4bjd7tItWinlH157DR58EAYMgM8+gwKZURp0bJ/zFB0dzeHDhwFI\nS0ujevXquFwu5s+fz7Zt5zSSqlIq1L34og3+q6+GqVNLPfghwIZ38AexsbF07tyZFi1a0K5dOzZs\n2EDLli1JSkriwgsvdLo8pZS/e/ppGDMGhg6Fjz6CCGdiWLy871pikpKSzMn94devX0/Tpk0dqijw\n6M9LKT9ijA39Z56xPXv+/e8S6c4pIiuNMUln20+bfZRSqqQZA6NH2+C/9dYSC/7zoc0+SilVkoyB\nhx6yN3jvugveegvCnL/udr4CpZQKVh4P3HuvDf7774e33/aL4Ae98ldKqZLh8cAdd9iHuB5+GF54\nAfxoPEv/+BWklFLBJDcXRoywwf+Pf/hd8INe+SullG+53TBsGEyeDE89Bf/8p9MVFUmv/H3gtttu\nY926dSV6jL59+3Lo0KFTvv7EE0/w8ssvl+ixlVLnKCfH9t+fPBmef95vgx/0yt8nJkyYUOLHmDlz\nZokfQynlhexsuO46OzjbK6/YJ3j9mF75n6cjR47Qr18/WrVqRYsWLfj0008LTdAyceJEGjduTPv2\n7bn99tu55557ABg+fDh33XUXHTp0oEGDBixYsIARI0bQtGlThg8ffuLzJ0+eTMuWLWnRogWjRo06\n8fWEhAT2798PwLPPPkvjxo3p0qULGzduLL2TV0oVLSsLBg+2wf/WW34f/BDAV/4PPACrfTuiM4mJ\ntkfWmXz77bfUrl2bb775BrDj+7z77rsA/Pnnnzz99NOsWrWK6OhoevToQatWrU5878GDB1myZAkz\nZsxgwIABLF68mAkTJtCuXTtWr15N9erVGTVqFCtXrqRy5cr07t2b6dOnM2jQoBOfsXLlSqZMmXJi\nSOk2bdrQtm1b3/4glFLn7uhRGDQI5s6F996zwzMHAL3yP08tW7Zkzpw5jBo1iuTkZGJiYk68t2zZ\nMrp160aVKlVwuVwMGVJ4INMrr7wSEaFly5bUqFGDli1bEhYWRvPmzdm6dSvLly+ne/fuVKtWjYiI\nCG644QYWLVpU6DOSk5O56qqrKFeuHBUrVjwxf4BSygEZGdCvnw3+998PmOCHAL7yP9sVeklp3Lgx\nq1atYubMmTz22GP07NnznL/3+FDOYWFhhYZ8DgsLw+1243K5fF6vUqqEpKdD376wZImdcP2GG5yu\n6Lx4feUvIvVEZL6IrBORX0Xk/iL26S4iaSKyOm8ZU9RnBYI///yTcuXKceONN/Lwww+zatWqE++1\na9eOhQsXcvDgQdxuN9OmTTuvz27fvj0LFy5k//795ObmMnnyZLp161Zon0suuYTp06eTmZnJ4cOH\n+e9//+uT81JKnYdDh+Dyy2HpUpgyJeCCH3xz5e8G/s8Ys0pEooGVIjLHGHNy38dkY0x/HxzPUb/8\n8gsPP/wwYWFhuFwu3n33Xf72t78BUKdOHR599FHat29PlSpVuPDCCws1C51NrVq1GDt2LJdeeinG\nGPr168fAgQML7dOmTRuuu+46WrVqRfXq1WnXrp1Pz08pdRYHDkDv3vDzz/D557a9PwD5fEhnEfkK\neMsYM6fA17oDfzuf8A/UIZ0zMjKoUKECbrebq666ihEjRnDVVVc5Uksg/LyUCij790OvXrB+vZ12\nsV8/pys6hSNDOotIAnZKx5+KeLujiKwRkVki0tyXx/UnTzzxBImJibRo0YL69esX6qmjlApge/bA\npZfCxo0wY4ZfBv/58NkNXxGpAEwDHjDGpJ/09iog3hiTISJ9gelAoyI+YyQwEiAuLs5XpZUqfdpW\nqSD055/Qsyds3w7ffAM9ejhdkdd8cuUvIi5s8H9ijPni5PeNMenGmIy87ZmAS0SqFrHfeGNMkjEm\nqVq1ar4oTSmlvLNjB3TrBikp8O23QRH84JvePgJMBNYbY145zT418/ZDRNrnHTfV22MrpVSJ2rrV\nBv/evTB7NnTt6nRFPuOLZp/OwE3ALyJy/JnbR4E4AGPMOOAa4C4RcQOZwPXGXycPVkopgC1b7FV+\nerp9iCvIetZ5Hf7GmB+AMw5UbYx5C3jL22MppVSp2LjRBn92Nnz/PbRu7XRFPqfDO5SQgoO9KaUC\nyLp1tqnH7Yb584My+EHD3yvGGDwej9NlKKV85eefoXt3O+vWggXQsqXTFZUYDf/ztHXrVpo0acKw\nYcNo0aIFH3/8MR07dqRNmzYMGTKEjIyMU76nQoUKJ7Y///zzQkM4K6X8xKpVth9/mTKwcCEE+QOS\nATuwm2NjOgO//fYbH374IQ0bNmTw4MHMnTuX8uXL88ILL/DKK68wZkzADl2kVGhatsyO1VOxom3q\nadDA6YpKXOCGv4Pi4+Pp0KEDX3/9NevWraNz584AHDt2jI4dOzpcnVLqvPz4I/TpA9Wq2Zu78fFO\nV1QqAjf8nRrTGShfvjxg2/wvu+wyJk+efMb98x5xACArK6tEa1NKnYdFi+ywzLVr2+CvW9fpikqN\ntvl7oUOHDixevJjNmzcDdorHTZs2nbJfjRo1WL9+PR6Phy+//LK0y1RKFWXePHvFHxdn2/hDKPhB\nw98r1apV44MPPmDo0KFcdNFFdOzYkQ0bNpyy39ixY+nfvz+dOnWiVq1aDlSqlCrk22+hf39o2ND2\n6gnB/y99PqSzrwTqkM7+RH9eShXhv/+Fa66BZs1gzhyoesowYwHNkSGdlVLKr33xBQweDBddZJt9\ngiz4z4eGv1IqNHz6KVx7rR2jZ+5cqFLF6YocFXDh76/NVP5Gf05KFfCf/8Bf/gKdOsF338F5TK8a\nrAIq/KOiokhNTdVgOwtjDKmpqURFRTldilLO+/e/YdgwO17PrFkQHe10RX4hoPr5161bl5SUFPbt\n2+d0KX4vKiqKuiHWdU2pU7z3Htx5p51w/csvoVw5pyvyGwEV/i6Xi/r16ztdhlIqELz5Jtx3n51r\n9/PPQf8SLiSgmn2UUuqc/L//Z4N/0CDbw0eD/xS+mMaxnojMF5F1IvKriNxfxD4iIm+IyGYR+VlE\n2nh7XKWUKtLzz8Pf/gZDhsBnn9lROtUpfHHl7wb+zxjTDOgA/FVEmp20zxVAo7xlJPCuD46rlFL5\njIEnn4RHH7U9eyZNApfL6ar8ltfhb4zZZYxZlbd9GFgP1Dlpt4HAR8ZaClQSkdB7nlopVTKMgcce\ngyeegOHD4aOPICKgbmmWOp+2+YtIAtAa+Omkt+oAOwq8TuHUXxBKKXX+jIFHHoHnnoPbb4eJEyE8\n3Omq/J7Pwl9EKgDTgAeMMenF/IyRIrJCRFZod06l1FkZYyd2evll+OtfYdw4CNN+LOfCJz8lEXFh\ng/8TY8wXReyyE6hX4HXdvK8VYowZb4xJMsYkVatWzRelKaWClccDd98Nb7wBDz5ou3Zq8J8zX/T2\nEWAisN4Y88ppdpsBDMvr9dMBSDPG7PL22EqpEJWba5t4xo2D0aNt184Ckyaps/PFHZHOwE3ALyJy\nfFLdR4E4AGPMOGAm0BfYDBwFbvHBcZVSocjthltuseP1jBljb/Jq8J83r8PfGPMDcMafvLGD8fzV\n22MppUJcTo4dp2fKFHjmGfjHP5yuKGBpXyilVGA4dgyGDrVP7L74Ijz8sNMVBTQNf6WU/8vOtk/s\n/ve/8NprcP8pAwmo86Thr5Tyb5mZdvatb7+Fd96Bu+5yuqKgoOGvlPJfR4/CgAHw/fcwYQLceqvT\nFQUNDX+llH/KyID+/SE5GT74wN7oVT6j4a+U8j/p6dC3Lyxdart0Dh3qdEVBR8NfKeVfDh6EPn1g\n1SrbpfOaa5yuKChp+Cul/Edqqp1y8ZdfYNo0296vSoSGv1LKP+zdC5ddBhs3wvTpttlHlRgNf6WU\n83btgl694I8/bF/+yy5zuqKgp+GvlHLWzp3Qo4ddz5wJ3bs7XVFI0PBXSjln+3Yb/Hv32oe4unRx\nuqKQoeGvlHLGH3/ApZfCoUMwezZ06OB0RSFFw18pVfp++81e8R85AvPmQdu2TlcUcjT8lVKla8MG\nG/w5OTB/PrRq5XRFIUnnPFNKlZ61a6FbNzsFowa/o3w1h+/7IrJXRNae5v3uIpImIqvzljG+OK5S\nKoCsWWPb+MPDYcECaNHC6YpCmq+afT4A3gI+OsM+ycaY/j46nlIqkKxcafvuV6hgR+hs2NDpikKe\nT678jTGLgAO++CylVJBZuhR69oSYGFi4UIPfT5Rmm39HEVkjIrNEpHlRO4jISBFZISIr9u3bV4ql\nKaVKxA8/2LF6qla1wV+/vtMVqTylFf6rgHhjTCvgTWB6UTsZY8YbY5KMMUnVqlUrpdKUUiViwQI7\nOmft2jb44+KcrkgVUCrhb4xJN8Zk5G3PBFwiUrU0jq2UcsCcOXZgtvh4+0ugTh2nK1InKZXwF5Ga\nIiJ52+3zjptaGsdWSpWyWbPgyiuhUSMb/DVrOl2RKoJPevuIyGSgO1BVRFKAxwEXgDFmHHANcJeI\nuIFM4HpjjPHFsZVSfmTGDBgyxHbjnD0bYmOdrkidhk/C3xhzxjnWjDFvYbuCKqWC1bRpcP310KaN\nHaStcmWnK1JnoE/4KqW8N3kyXHcdtG9v2/s1+P2ehr9SyjsffQQ33gidO8N330HFik5XpM6Bhr9S\nqvgmToThw+2wDTNn2id4VUDQ8FdKFc+778Jtt8Hll9upF8uXd7oidR40/JVS5+/11+Huu22XzunT\noWxZpytS50nDXyl1fl56CR54AAYPhs8/h8hIpytSxaDhr5Q6d88+C488Yrt0TpkCZco4XZEqJg1/\npdTZGQOPPw6PPQY33QQffwwul9NVKS/oNI5KqTMzBh59FMaOhREjYPx4OyGLCmga/kqp0zMG/u//\n4NVX4c474e23IUwbDIKB/ldUShXN44H77rPBf9998M47GvxBRK/8lVKn8njslf6//mWv/F96CezA\nvCpI6K9xpVRhublw6602+B99VIM/SOmVv1Iqn9sNN98MkybBk0/CP/+pwR+kNPyVUlZODtxwA0yd\nCs89B3//u9MVqRKk4a+UgmPH7JDM06fDyy/bdn4V1HzS5i8i74vIXhFZe5r3RUTeEJHNIvKziLTx\nxXGVUj6QlWWHapg+Hd54Q4M/RPjqhu8HQJ8zvH8F0ChvGQm866PjKqW8kZkJAwfCN9/AuHFw771O\nV6RKia+mcVwkIgln2GUg8FHevL1LRaSSiNQyxuzyxfGVUsVw5IgdlXPBAnj/fbjlFqcrCkjGGNwe\nN8dyj5HjycHtcZOTm7fOe+32uMn15OZvm9wTX8s1uSfWHuMh15NLTFQMl8RfUqJ1l1abfx1gR4HX\nKXlfKxT+IjIS+5cBcXFxpVSaUiHo8GHo1w8WL86fiSvIZLmzSMtKIy07jfTsdDKOZRS5HM05SmZO\npl27M8l02+0sdxbZ7my7zs0+sX0s91ihJceT4/PaL65zMUtvW+rzzy3Ir274GmPGA+MBkpKSjMPl\nKBWc0tLgiitg2TLbpfO665yu6KwyczLZnbGbXRm72J2xm71H9pJ6NJXUzFT2H91PamYqqUdTOZB5\ngLTsNNKy0sjOzT6nzw6XcMq5ylHWVdauI8pS1lWWshFliYyIJCYqhsjwSKIiooiMiKRMWBm7Di9D\nmfAyuMJcdh3uwhXmwhXuIiIsAleYXRdcwsPC87clnPCwcMIIx33MRfbRvCXTRXRUuRL+iZZe+O8E\n6hV4XTfva0qp0nTwIPTuDWvW2C6dV13ldEVk5mSyPW37iWVb2rYT2zsP72R3xm7Ss9OL/N7yrvJU\nLVeV2HKxxJaNJaFSApWiKhETGUNMVAwxkTFUiqpExciKREdGU6FMBSqUqUB5V3m7LlOeMuHFG5ba\nGNtyduiQ/bEeOgRpB+3v1rQ02Jeev334MKSn23XB7YwMu5iTLnU7dID+S4pV1jkrrfCfAdwjIlOA\ni4E0be9XqpTt3w+XXQbr1sG0aba9v5R4jIdth7axMXUjm1I3sXH/RjYd2MSm1E1sT9teaN8wCaNO\ndB3iYuJoXbM1NSvUPGWpXr46sWVjiYzwzUQymZn2x7Nvn13v3w+pqXY5cMAuBbcPHbKL233mz3W5\n7Hz2MTEQHW2X6tXhggvyX1eocOpSs6ZPTuuMfBL+IjIZ6A5UFZEU4HHABWCMGQfMBPoCm4GjgN5Z\nUqo07d0LvXrBpk3w1VfQ50yd87yTmZPJr/t+ZfXu1azevZo1e9awZvcaDh87fGKfmMgYmlRtwiXx\nl9CoSiPqV6pPfKV44mPiqR1dG1e4d3MFGGOvuHftssvu3fZHsGePXRfc3rcPjh49/WdVqgRVqtgl\nNhYaNIDKle3XC65jYux2TEx+4EdF+e8D0r7q7TP0LO8b4K++OJZS6jzt2gU9e8LWrbZLZ8+ePv34\nlPQUFm9fzOIdi/lh+w/8vOdnck0uANFlomlVsxU3t7qZi2pcRNNqTWkc25hq5aohxUzFzExISYGd\nO4tedu+2S1bWqd/rctkr7+rVoUYNuPBCu121av5SrZpdV6liQz3Cr+6M+k6QnpZSCrAp2aMH/Pkn\nzJoF3bp5/ZE703cya/MsFmxdwA/bf2Bb2jYAyrnK0aFuB0Z3GU3rmq1JrJlI/cr1CZNzf5zIGNvk\n8scf9nfVtm2wYwds356/3r//1O+LjoY6dezSpQvUqmWbTmrVyt+uUcNemfvrlXhp0/BXKlht22aD\nf98+mD0bOnUq1sfk5Obw444fmfnbTGZtnsUve38BoFaFWnSJ68KDHR6kc1xnWtVodU7NNZmZ8Pvv\nsGVL/nI87LduPbUJpmJFiIuDevWgXTu7rlcvP+zr1LHhr86Phr9Swej33+HSS223krlzoX378/r2\nLHcWs36bxeS1k/luy3ekZ6fjCnPRJa4LL/Z6kb6N+tKsWrPTNt1kZ9tQ37Sp8LJli/0jpKCKFW07\neuPGcPnlkJBgl/r1bejHxBTvR6DOTMNfqWCzaZO94s/MhHnzoM25DaXl9riZ/8d8Jq+dzLT100jP\nTqd6+epc1/w6+jbqS8/6PYmOLHyJnZoK69cXXjZutH90eDz5+9WoAY0a2c5GF1xQeImN1aYYJ2j4\nKxVM1q+3we92w/z5cNFFZ/+Wfet5b+V7TFk7hT1H9lAxsiKDmw5maIuh9Kjfg4iwCA4ehDXLYe1a\n+PXX/PW+ffmfExUFTZrAxRfDsGH2Sr5xYxv6evXufzT8lQoWv/xie/KEhcHChdCs2Wl3zfXk8vWm\nr3lz2ZvM+2MeZcLLcGXjK7n2whu4wNOXDb9GMm88vLLGBv3OAo9kRkdD8+b2MYFmzaBpU7vEx+sU\nv4FEw1+pYPC//9k2lchI+P57ewlehP1H9zNx1UTeXfEu2/amUv1wb/qHzabCga5s/CKKm361Q/sD\nlCljw71nTxv2LVrYpV49baYJBhr+SgW65cvtkA0VK9rgv+CCU3ZZv2MXj/7nc75euBt3SkvKpiYj\ne+qy1whfY/u6t25tnwNr1couTZrYfvEqOGn4KxXIliyxT+vGxtrgT0jgyBFYtQpWrIDkJZnM//Ew\nh3bWAuxY/bXrHePijmVo3dreC27TxvaFV6FFw1+pQJWcjOnbl2OxtfjiznnMf64eP/1k2+iP97SR\niqmYOstp093DfVd1on/3WsTGFm8gMxVcNPyVCiD79sHSpbBn8vfc8OmVbDdxXJoxj12jalO5MrRt\nl0vnVsks402O1VjCTZ0v45+X/JOGVRo6XbryMxr+Svmp3FzbnXLxYtu6s2QJbN4MvfmO6QxiZ9mG\nfDh0Li9eWoN27Qxrc7/kodkPsj1tO9c1v46nLl1A49jGTp+G8lMa/kr5iYwM+OknG/aLF9sr/PS8\nYexr1ICOHWFsl6+56pOroWkzGs6bw3NVq7Jh/wbunXUfc36fQ8vqLVlw8wK6JXg/ho8Kbhr+Sjlk\n924b8snJ8MMPsHq1vdoXsV0q//IX6NzZDslTvz7IV9Ph2mvtg1uzZ3O4vIun5zzCq0tfpbyrPG/0\neYO72t1FRJj+b63OTv+VKFUKjLHj2iQn5y+bN9v3ypa1T8X+/e92RMqLL7ajTxYydar9bZCUBLNm\n8f3BVdzy0S1sT9vOiMQRPN/reaqXr17q56UCl68mc+kDvA6EAxOMMWNPen848BL5Uze+ZYyZ4Itj\nK+WPPB7bXr9okQ36RYvssPpge2V26QJ33GHXbdrYB6pOa9IkuOkm6NSJo9On8vclj/PGsjdoHNuY\nxSMW06le8UbrVKHN6/AXkXDgbeAyIAVYLiIzjDHrTtr1U2PMPd4eTyl/5Hbbh2wXLcoP/IMH7Xt1\n69oBNrt2hUsusROInPMwCB98ACNGQLduLB83hhsnd2NT6ibubX8vY3uNpZyr5Cf6VsHJF1f+7YHN\nxpjfAfLm6R0InBz+SgWN7Gz7YO3xsF+82N6wBTuQ2eDBNuy7dbNj3hRrOITx4+GOO/D06skzDybx\n5Ke9qBNdh7k3zaVnA9/OxqVCjy/Cvw6wo8DrFOwk7Se7WkQuATYBDxpjdhSxj1J+6cgR2/tm0SI7\nZtrSpfYXANhxb4YNs0HftauPnpZ9+2245x6OXtadngNTWbr8BYYnDue1y18jJkqHyFTeK60bvv8F\nJhtjskXkDuBDoMfJO4nISGAkQFxcXCmVptSp0tLs1fzxK/vly23TTliYHQPn7rttE07XrrYN36de\nfRUeeojdPS+mVbf/kZMRxpfXfcmgCwf5+EAqlPki/HcC9Qq8rkv+jV0AjDGpBV5OAF4s6oOMMeOB\n8QBJSUnGB7UpdU727bPdLY+H/erV9qZtRISdOvChh+yVfefOJTw2/QsvwOjRrL3kQlp3+omLqrXh\n8yGfU79y/RI8qApFvgj/5UAjEamPDf3rgb8U3EFEahlj8vo6MABY74PjKlVs27cX7omzYYP9elSU\nfZjqn/+0V/YdOkC50rqn+vTTMGYM8zrW4PJuG7gl6Tbe7PsmURFRpVSACiVeh78xxi0i9wDfYbt6\nvm+M+VVEngJWGGNmAPeJyADADRwAhnt7XKXOlccD69blP0yVnAw78u44xcTYq/nhw20TTtu2dkj8\nUmUMjBkDzzzD1KRyDL/iEOP7T2RE6xGlXIgKJWKMf7auJCUlmRUrVjhdhgpAmZl2OOPFi23YL14M\nhw7Z92rWtCHfpYu9sm/ZEsLDHSzWGBg9Gl58kYltheduiOfz67+gda3WDhalApmIrDTGJJ1tP33C\nVwW83bvhxx/zx8RZtQpycux7TZrANdfYsO/aNW+YBH+ZhcoYzAMPIG+8wdvt4Ku/9mDFtVOpXLay\n05WpEKDhrwJKTg6sWZM/yuWSJbB1q30vMjL/5mznzrbtvmpVR8s9PY8H9913EvHev3i1A6z/+218\n0+8dXOE6dZYqHRr+ym8ZY2/MLltmR7tctsw252Rm2vfr1LEBf++9dvCzsw6T4C88HjJvHUbZDz7h\nxc4Q9sKLvNfpb4jf/EmiQoGGv/IbBw7YcF++PD/w9+yx70VG2v71d9xhA79jRzuReMDJzeXQDVdT\n6dOveL57BE3ensLgZlc7XZUKQRr+yhFpaXYsnONhv2IF/P57/vtNmsDll0P79naUy4suCpCr+jNx\nu9l9dR9qzpjHi5dXoNfE72lI469ZAAARxUlEQVRXp53TVakQpeGvStzevTboV63KX2/Zkv9+QoId\nqXjkSLtu27aIIY0DXU4OKf27UXf2El4ZWJ3rP1xOXIw+xa6co+GvfMbtho0b4eef7U3Z48vxoYwB\nGjSwbfMjRth127ZQrZpzNZeK7Gy2Xt6BhIWref3aeIZ/sIoqZas4XZUKcRr+6rwZYx+SWru28LJu\nXf5gZy4XNGsGl10GrVrZoE9MDMIr+rMwmZls6dGahks38s7Nzbht/DLKlynvdFlKafir08vNtd0o\n168vvPz6Kxw+nL9fnTp2ZMt777VBf9FFdsz6gG+j95LnSAa/XdKCRqu2MeHO9tz+1g/alVP5DQ1/\nRWoqbNpUeNm40a6PX8mDnUS8aVM7fHGLFnZp3hwq6zNJp8hJO8jmLk1psnYPkx7qxYiXvyNMznUG\nF6VKnoZ/CDDGPgW7ZUv+snlz/vrAgfx9w8Ntu3zjxra3TdOm9iq+aVMN+XOVdWAvWzo1pcmmA3z1\n2DXc8NRn2odf+R0N/yDg8dgeNdu322aagssff8C2bfkPRoEdkz4+Hi64AIYMsd0qGze2S0KCba9X\nxXN0359s7dSMxr+nMffZEVz194lOl6RUkTT8/dzxYN+5s/Cyfbu96Xp8fXwsm+OqVLHj2DRvDv36\n2e0LLrBLfLy2x5eEw7u2sbNTCxpuz2DRy/fS58E3nC5JqdPS8HfI0aM21Pfsscvu3bZL5K5dhbd3\n7bJdKAsKD7c3WePi7ANQQ4bY7Xr17JV7fDxUrOjIaYWsQzs2s7dTKxJ2H+XHNx+m591FzleklN/Q\n8PeB3Fw7ZPCBA/bm6f79hZd9++x679785fhk3yerWtXOAVuzpm1rr1Mnf6lb166rV3d4GGJVyP7f\nf+VQ1yTq7c1i5bjH6X7rE06XpNRZafhjm1YOH4b0dDvswMnLoUNw8KBdH98+vqSm5o8VXxSXyz7E\nVLWqXXfoYMO7Rg27Pr5dq5bd1uaYwLJn0//IuKQDtQ4cY+37Y+l80yinS1LqnPgk/EWkD/A6diav\nCcaYsSe9Hwl8BLQFUoHrjDFbfXHskx05Ah99ZK+si1oOH85f0tPt+nRX4QW5XLa3y/GlalV7gzQ2\n1ravF1yqVs1foqP9aPx45Tvp6aS9/CxlXnmZ6jkeNv3nddpde5/TVSl1zrwOfxEJB94GLgNSgOUi\nMsMYs67AbrcCB40xDUXkeuAF4Dpvj12UzEy4++781+XLQ4UK+Ut0tL0Cb9DAblesaNfR0XZKv5OX\nihVt2JctqyGusH/uvf46ua+9SkxaOrMbR1D9jfdpffnNTlem1HnxxZV/e2CzMeZ3ABGZAgwECob/\nQOCJvO3PgbdEREwJzCFZpZKH3evTKF/eTrwd5ovnarLzFhW6MjLg3Xfhrbfg8GHmtCzL2G4VeGn0\n9yTqyJwqAPki/OsAOwq8TgEuPt0+eRO+pwGxwH4fHL+QsIOp1Gha3dcfqxSIkD6gD4MbrmB1dQ9z\nh80lsWai01UpVSx+dcNXREYCIwHi4oo53G358vDaaz6sSikgLIzf2iTQdcntGIQFwxbQonoLp6tS\nqth8Ef47gYJzKtXN+1pR+6SISAQQg73xW4gxZjwwHiApKal4TULlysH99xfrW5U6nTW719Dr4164\nwlzMGzaPptWaOl2SUl7xRYv4cqCRiNQXkTLA9cCMk/aZARy/I3YN8H1JtPcrVRJW/LmCHh/1ICoi\nioXDF2rwq6Dg9ZV/Xhv+PcB32K6e7xtjfhWRp4AVxpgZwETgYxHZDBzA/oJQyu8t3r6YvpP6UqVs\nFeYNm0eDyg2cLkkpn/BJm78xZiYw86SvjSmwnQUM8cWxlCot3//xPVdOvpK6Fesyb9g86las63RJ\nSvmMDjCuVBFm/jaTvp/0pUHlBiwcvlCDXwUdDX+lTvLF+i8YNGUQzas3Z8HNC6hZoabTJSnlcxr+\nShUw6ZdJXDv1WpJqJzFv2Dxiy8U6XZJSJULDX6k841aM48YvbqRrfFdm3zSbSlEhNtu8Cika/irk\nGWN4euHT3PXNXfRt1JeZf5lJhTIVnC5LqRLlV0/4KlXaPMbDA98+wJvL3mRYq2FMuHICrnCdx1IF\nPw1/FbJycnMY/tVwJv0yiYc6PMRLvV8iTPSPYRUaNPxVSDpy7AhDpg5h1uZZPN/zeUZ1HoXomN0q\nhGj4q5BzIPMA/Sf156edPzG+/3hub3u70yUpVeo0/FVI2XxgM/0m9WProa1MHTKVwU0HO12SUo7Q\n8FchI3lbMoM+HYQgzL1pLl3juzpdklKO0btbKiT85+f/0OvjXlQtV5Wlty3V4FchT8NfBTVjDE8s\neIKbvryJTvU6seTWJTSs0tDpspRynDb7qKCV5c7i1hm3MumXSQxPHM57/d+jTHgZp8tSyi9o+Kug\ntCNtB0OmDuGnnT/xXI/nGN1ltHblVKoADX8VdOb+Ppeh04aS7c5m2rXTtEePUkXwqs1fRKqIyBwR\n+S1vXfk0++WKyOq85eQpHpXyCY/x8OyiZ+n9cW9qVqjJipErNPiVOg1vb/iOBuYZYxoB8/JeFyXT\nGJOYtwzw8phKneJg5kEGTB7AY/MfY2jLoSy9dSmNYxs7XZZSfsvbZp+BQPe87Q+BBcAoLz9TqfPy\nv13/4+rPriYlPYW3+77NXUl3afu+Umfh7ZV/DWPMrrzt3UCN0+wXJSIrRGSpiAw63YeJyMi8/Vbs\n27fPy9JUsHN73Dyf/DwXT7iYHE8Oybckc3e7uzX4lToHZ73yF5G5QFHz2P2j4AtjjBERc5qPiTfG\n7BSRBsD3IvKLMWbLyTsZY8YD4wGSkpJO91lK8Vvqb9w8/WaWpCxhSLMhvNPvHaqWq+p0WUoFjLOG\nvzGm1+neE5E9IlLLGLNLRGoBe0/zGTvz1r+LyAKgNXBK+Ct1Nh7j4d3l7/LI3EcoE16GSYMncX2L\n6/VqX6nz5G2zzwzg5rztm4GvTt5BRCqLSGTedlWgM7DOy+OqELQjbQeX/+dy7pl1D13jurL2rrUM\nbTlUg1+pYvD2hu9Y4DMRuRXYBlwLICJJwJ3GmNuApsB7IuLB/rIZa4zR8FfnzO1x8/aytxmzYAy5\nnlzG9RvHyLYjNfSV8oJX4W+MSQV6FvH1FcBteds/Ai29OY4KXQu2LuDeWfeydu9ael/Qm3f6vsMF\nVS5wuiylAp4+4av8Ukp6Cg/PeZgpa6cQHxPPl9d9ycAmA/VqXykf0fBXfiUzJ5PXf3qdZxY9Q67J\n5fFujzOq8yjKuso6XZpSQUXDX/mFLHcWE1ZN4Lnk59iVsYtBFw7ild6vUL9yfadLUyooafgrR2W7\ns5n4v4k8l/wcOw/v5JL4S5h09SS6J3R3ujSlgpqGv3JEtjubD1Z/wLPJz7IjfQed63Xmw0Ef0qN+\nD23XV6oUaPirUpWSnsJ7K95j/Krx7D2yl451OzJxwER6Neiloa9UKdLwVyXOGEPy9mTeWvYWX6z/\nAo/x0L9xf+5tf6+GvlIO0fBXJWbvkb1M/XUq41eN5+c9P1M5qjIPdniQu9vdrTdylXKYhr/yqfTs\ndKZvmM6kXyYx9/e55JpcEmsmMuHKCQxtOZRyrnJOl6iUQsNf+UBaVhqzt8zms3Wf8fWmr8lyZ5FQ\nKYFHOj/C0BZDaVlDH/BWyt9o+KvzZozh5z0/M2vzLGZtnsXi7YvJNblUL1+d29vcztAWQ+lQt4O2\n5SvlxzT81VkZY9h8YDOLdywmeVsy3235jp2HdwKQWDORUZ1HcUWjK+hQtwMRYfpPSqlAoP+nqlNk\n5mSyZs8aFm9fzOIddtl7xE7VUDmqMr0a9OKKhlfQp2EfakXXcrhapVRxaPiHuL1H9rJm9xpW717N\n6j2rWb17NRv2b8BjPABcUPkC+jTsQ5d6Xegc15kLq15ImHg7DYRSymka/iHgaM5RthzYwqbUTXY5\nsOnE9v6j+0/sFxcTR2LNRK5uejWJNRPpVK8TNSsUNYOnUirQafgHuIxjGezO2H1i2ZG2g21p29ie\ntv3EumDAA9SOrk3j2MZc3fRqmsQ2IbFmIq1qtqJK2SoOnYVSqrR5Ff4iMgR4AjtbV/u8SVyK2q8P\n8DoQDkwwxoz15rjByO1xk5aVRlp2GoeyDpGWZdepmamkHk09sd6fuZ/Uo6nsObKH3Rm7yTiWccpn\nlXeVJ75SPPEx8bSr3Y64mDjqV6pPk6pNaFSlEdGR0Q6coVLKn3h75b8WGAy8d7odRCQceBu4DEgB\nlovIDH+bytEYQ67JJSc3B7fHTY4nb52bw7HcY+R47Lrgku3OJsudRXZudqHtzJxMjuYcJdOdWWj7\nSM4Rjhw7QsaxjBPLkZwjpGenczTn6BnriwyPJLZcLLFlY4ktF0tS7SRqVahFzQo1Cy31KtajUlQl\n7WaplDojb6dxXA+cLWjaA5uNMb/n7TsFGEgJTeKeejSVrv/uSq7JxWM85HpyyTW5hdZuj5tck7cu\n8NrXIsIiKBtRlrKuspSNKEuFMhVOLLHlYu22qwLRkdHERMYQExVTaF0pqtKJwC/nKqeBrpTymdJo\n868D7CjwOgW4uKgdRWQkMBIgLi6uWAdzhbtoXr054RJOmIQRHhZOuOQtedsRYRFEhEUQHma3j3/N\nFe6y6zBXoddlwssUWlxhLsqElyEyIpKoiCgiw/PWEZFEhkeeCHtXuKtY56CUUiXtrOEvInOBorp8\n/MMY85UvizHGjAfGAyQlJZnifEbFyIpMHTLVl2UppVTQOWv4G2N6eXmMnUC9Aq/r5n1NKaWUQ0rj\naZ3lQCMRqS8iZYDrgRmlcFyllFKn4VX4i8hVIpICdAS+EZHv8r5eW0RmAhhj3MA9wHfAeuAzY8yv\n3pWtlFLKG9729vkS+LKIr/8J9C3weiYw05tjKaWU8h0dpEUppUKQhr9SSoUgDX+llApBGv5KKRWC\nxJhiPUtV4kRkH7DNi4+oCuw/617+L1jOA/Rc/FWwnEuwnAd4dy7xxphqZ9vJb8PfWyKywhiT5HQd\n3gqW8wA9F38VLOcSLOcBpXMu2uyjlFIhSMNfKaVCUDCH/3inC/CRYDkP0HPxV8FyLsFyHlAK5xK0\nbf5KKaVOL5iv/JVSSp1G0Ia/iDwtIj+LyGoRmS0itZ2uqbhE5CUR2ZB3Pl+KSCWnayouERkiIr+K\niEdEAq5nhoj0EZGNIrJZREY7XY83ROR9EdkrImudrsUbIlJPROaLyLq8f1v3O11TcYlIlIgsE5E1\neefyZIkdK1ibfUSkojEmPW/7PqCZMeZOh8sqFhHpDXxvjHGLyAsAxphRDpdVLCLSFPBg533+mzFm\nhcMlnbO8+ag3UWA+amCov81Hfa5E5BIgA/jIGNPC6XqKS0RqAbWMMatEJBpYCQwKxP8uYudqLW+M\nyRARF/ADcL8xZqmvjxW0V/7Hgz9PeSBgf8sZY2bnDY0NsBQ7IU5AMsasN8ZsdLqOYjoxH7Ux5hhw\nfD7qgGSMWQQccLoObxljdhljVuVtH8YOHV/H2aqKx1gZeS9deUuJZFfQhj+AiDwrIjuAG4AxTtfj\nIyOAWU4XEaKKmo86IEMmWIlIAtAa+MnZSopPRMJFZDWwF5hjjCmRcwno8BeRuSKytohlIIAx5h/G\nmHrAJ9gJZfzW2c4lb59/AG7s+fitczkXpXxNRCoA04AHTvrLP6AYY3KNMYnYv/Dbi0iJNMl5NZmL\n085jfuFPsJPJPF6C5XjlbOciIsOB/kBP4+c3anww77O/0vmo/VRe+/g04BNjzBdO1+MLxphDIjIf\n6AP4/KZ8QF/5n4mINCrwciCwwalavCUifYBHgAHGmKNO1xPCdD5qP5R3k3QisN4Y84rT9XhDRKod\n780nImWxnQtKJLuCubfPNKAJtmfJNuBOY0xAXqWJyGYgEkjN+9LSAO65dBXwJlANOASsNsZc7mxV\n505E+gKvAeHA+8aYZx0uqdhEZDLQHTuC5B7gcWPMREeLKgYR6QIkA79g/38HeDRv+tiAIiIXAR9i\n/32FYec8f6pEjhWs4a+UUur0grbZRyml1Olp+CulVAjS8FdKqRCk4a+UUiFIw18ppUKQhr9SSoUg\nDX+llApBGv5KKRWC/j+3uXalz1k0gQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x = np.linspace(-3, 3)\n",
    "    tanh = tf.nn.tanh(x).eval()\n",
    "    sigmoid = tf.nn.sigmoid(x).eval()\n",
    "    relu = tf.nn.relu(x).eval()\n",
    "    \n",
    "    plt.plot(x,tanh, 'g', x,sigmoid, 'b', x,relu, 'r')\n",
    "    plt.legend(('tanh',  'sigmoid',  'relu'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fO338uTSiQk4"
   },
   "source": [
    "這些Activation Function的使用時機可以簡單這樣說，當我們想要輸出值在+1和-1之間時使用tanh，而當我們想要一個輸出值衡為正時使用sigmoid，它可以將輸出值壓在+1和0之間。tanh比sigmoid多了兩個好處，第一，tanh的梯度變化大於sigmoid，有利於訓練效率，第二，tanh的輸出均值為0，可以避免將前層的梯度偏差帶到下一層。\n",
    "\n",
    "不過，以上的這兩種Activation Function在非常深的網路都會有一個共通問題—梯度消失，仔細看上圖，tanh和sigmoid在極大和極小的地方都會彎成平的，所以每過一次這種Activation Function，訊號就會減小一點，當我們在深網路做Backpropagation時，訊號在過程中不斷的被磨損，到了前面的幾層就已經耗損完畢，此時更新的梯度近乎0，也就是梯度消失，那麼前面的這些層就再也訓練不到了。\n",
    "\n",
    "Relu正可以解決梯度消失的問題，如上圖，在正的部分Relu是線性的，所以多少訊號進來就多少訊號出去，如此一來就不會有耗損的問題，但特別注意，因為tanh和sigmoid會將輸出值限制在一個範圍內，所以有Normalization的味道，但是Relu沒有限制，Normalization可以使我們訓練的效率提升（因為梯度的方向可以直指低點），所以Relu常常會搭配Normalization Layer一起使用，來額外做Normalization，或者是最近一篇Paper提到的一種新的Activation Function：SELU，類似於Relu但是輸出值會是Normalize過的，非常神奇，在這邊我不多論述。\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "在上一回當中，我們的Gradient Descent採用的是將所有的Data一次全考慮進去，評估完所有的Data在一次性的更新權重參數，這樣的作法好處是比較穩定，因為我考慮的是真正的Training Set的$E_{in}$，但缺點就是計算時間長，因為要考慮所有Training Set的每筆數據，需要做大矩陣的計算，而且通常Training Set的數據量也不可以太小，這麼一來計算時間就會拉的很長。\n",
    "\n",
    "因此，有另外一種作法一次只考慮「一筆」數據，使用一筆數據來評估並更新權重參數，每一筆雖然更新結果都不怎麼準確，但是當我隨著時間看過整個Training Set後，就會有平均的效果，所以最後只要Learning Rate不要太大，最後的結果還是可以朝向最佳解的，這個手法會使得Gradient Descent具有隨機性，因此又被稱為Stochastic Gradient Descent，它所帶來的優點是計算時間變短了，我們將可以避免去涉及大矩陣的運算，但缺點是一次只評估一筆數據，將會非常的不穩定。\n",
    "\n",
    "另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Mini-Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，Mini-Batch Gradient Descent一次評估k組數據，並更新參數W，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的架構設計是非常有利於矩陣計算的，因為GPU會利用它強大的平行化將矩陣運算中每個元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Mini-Batch Gradient Descent，這個k值不要超過GPU平行計算所能容納的最大上限，這是個既有效率又更為穩定的作法，順道一提Tensorflow是可以支援GPU的計算的。\n",
    "\n",
    "實務經驗告訴我們Mini-Batch Gradient Descent雖然穩定性比Gradient Descent差，但是收斂的速度卻一點都不輸給Gradient Descent，原因就出在更新的次數，Mini-Batch Gradient Descent一次看的數據筆數比較少，所以Mini-Batch一個Epoch可以更新參數好幾次，而Gradient Descent卻只能更新一次，Mini-Batch的靈敏性，使得它的收斂速度更為快速。打個比方，就好像是兩艘船在搜尋小島，Gradient Descent像是巡洋艦，它有更好設備可以有更好的觀測能力，但是因為它的笨重造成它反應不夠靈敏，Mini-Batch Gradient Descent就像是小船一樣，雖然觀測設備沒這麼好，但是反應靈敏，卻是可以更容易率先找到小島。\n",
    "\n",
    "那我們來看看要怎麼做到Mini-Batch Gradient Descent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snzasuMGiQk5"
   },
   "outputs": [],
   "source": [
    "def fit(self, X, y, epochs=10, validation_data=None, test_data=None, batch_size=None):\n",
    "    ...略...\n",
    "\n",
    "    self.sess.run(self.init_op)\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %2d/%2d: ' % (epoch+1, epochs))\n",
    "\n",
    "        # mini-batch gradient descent\n",
    "        index = [i for i in range(N)]\n",
    "        random.shuffle(index)\n",
    "        while len(index) > 0:\n",
    "            index_size = len(index)\n",
    "            batch_index = [index.pop() for _ in range(min(batch_size, index_size))]\n",
    "\n",
    "            feed_dict = {\n",
    "                self.train_features: X[batch_index, :],\n",
    "                self.train_labels: y[batch_index],\n",
    "            }\n",
    "            _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "\n",
    "            print('[%d/%d] loss = %9.4f     ' % (N-len(index), N, loss), end='\\r')\n",
    "\n",
    "        ...略..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Swx_CYW9iQk-"
   },
   "source": [
    "在每一個`epoch`都完整的看過數據一遍，而mini-batch gradient descent是隨機取`batch_size`筆數據來更新權重，所以我採用這樣的作法，先依照數據的筆數`N`列出可能的Index有哪些，然後再做一個`random.shuffle`來做到隨機採樣，然後接下來只要簡單的從前面取`batch_size`筆數據進行更新，直到用盡所有的index為止，就可以做到mini-batch的效果。\n",
    "\n",
    "### Regularization\n",
    "\n",
    "當你開始加深你的DNN時，就已經在增加Model的複雜度，增加複雜度想當然爾的可以增加對於數據的描述能力，在分類問題中代表可以增加精確度，不過要特別注意Overfitting的出現，當Model越複雜越容易產生Overfitting，Overfitting的結果是有看過的數據描述的很好，但沒看過的數據預測就很差，所以在Training的過程要特別注意Validation Set的表現，如果發現Training Set的表現越來越好，但是Validation Set的表現裹足不前甚至變的更差，那就很有可能已經Overfitting了。\n",
    "\n",
    "如果你已經看到Overfitting出現了，有什麼方法可以抑制他呢？這個時候就需要Regularization的幫忙，在Neurel Network常見的Regularization有兩種：Weight Regularization和Dropout，待會會一一介紹。\n",
    "\n",
    "這邊特別注意，不要每次精確度沒有提升就怪Overfitting！如果連你的Training Set都沒辦法有好的表現，這就可能不是Overfitting，反而可能是Underfitting，這個時候不要再增加Regularization，而是試著去調整Learning Rate，或者增加模型的複雜度，加深DNN或增加神經元的數目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVC3-3zqiQk_"
   },
   "source": [
    "### Weight Regularization\n",
    "\n",
    "我們可以藉著在Loss Function裡頭加入Weight的貢獻，來達到限制W的大小的目標，這樣做可以降低Overfitting，詳細原理請[參考這篇的Regularization的部分](http://www.ycc.idv.tw/YCNote/post/28)。\n",
    "\n",
    "我們來看看應該怎麼做到L2 Regularization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81v73YhqiQlB"
   },
   "outputs": [],
   "source": [
    "# regularization loss\n",
    "regularization = tf.reduce_mean([tf.nn.l2_loss(w) / tf.size(w, out_type=tf.float32) for w in self.weights.values()])\n",
    "# total loss\n",
    "loss = original_loss + alpha * regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUst-NieiQlG"
   },
   "source": [
    "在這裡我習慣將Loss對Weights的個數做平均，這有一個好處當我在調整神經元數目時，`alpha`可以不需要大動作調整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXqjyBi1iQlH"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout是Deep Learning常用的Regularization技巧，它的作法是在訓練的時候我先隨機把部份的神經元關閉，使用較少的神經元訓練，來達到Regularization的效果，最後在「推論」的時候再使用全部的神經元，我個人覺得這有Aggregation Model的味道，分別訓練出許多的sub-model再做Aggregation以達到截長補短的效果。\n",
    "\n",
    "實作上有一些細節必須要注意，當我們關閉一些神經元時，也就是等於減少部份的貢獻量，所以我們需要依照相應比例來給予權重，以抵銷減少的部分。舉個例子，假設今天原本應該要輸出的值有十個，這十個值都是1，然後因為Dropout，變成五個1五個0的輸出，我們看到原本貢獻量因為Dropout一半而少一半，這樣並不合理，會導致我後面的Weights在更新的時候低估更新量，所以我們必須要將「沒被Dropout的部分」權重乘上一倍，才可以解決問題。因此，如果Dropout $r$倍的神經元，權重就要乘以$(1/r)$倍，我們來看看Tensorflow怎麼做的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "ROSeycZsiQlH",
    "outputId": "475dec3d-2e70-4179-858a-3f4dbdbc5538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original S =\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3.]\n",
      " [5. 5. 5. 5. 5. 5. 5. 5.]]\n",
      "Dropout S =\n",
      "[[ 2.  0.  0.  2.  0.  0.  2.  0.]\n",
      " [ 0.  0.  6.  0.  0.  6.  0.  6.]\n",
      " [10. 10. 10.  0. 10. 10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    S = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                           [3, 3, 3, 3, 3, 3, 3, 3],\n",
    "                           [5, 5, 5, 5, 5, 5, 5, 5]],tf.float32)  # 3 Data, 8 dim. Score\n",
    "    print('Original S =')\n",
    "    print(S.eval())\n",
    "    S_drop = tf.nn.dropout(S,keep_prob=0.5)  # dropout ratio = 1 - keep_prob = 0.5\n",
    "    print('Dropout S =')\n",
    "    print(S_drop.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-57xoU3iQlL"
   },
   "source": [
    "我們看到因為dropout 0.5倍，所以輸出值權重乘上2倍，另外一提，Tensorflow的Dropout機制是隨機的，所以Drop out的比例會接近我們想要的比例，但不是絕對剛好。\n",
    "\n",
    "那我要怎麼把Dropout放進去我的Model呢？特別注意，我們並不希望已經Training完的Model還有Dropout這一層，所以我在`structure`裡頭設計一個`train`的開關，當我在Training過程就把它打開，Dropout這一層就會被加進去，「推論」的時候就關閉，保持原有的神經元數量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tORdcAy4iQlN"
   },
   "outputs": [],
   "source": [
    "    def structure(self,features,labels,n_hidden,activation,dropout_ratio=0,train=False):\n",
    "        \n",
    "        ... 略 ...\n",
    "        \n",
    "        # layer 1\n",
    "        fc1 = self.getDenseLayer(features,self.weights['fc1'], self.biases['fc1'], activation=activation)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1, keep_prob=1-dropout_ratio)\n",
    "            \n",
    "        # layer 2\n",
    "        logits = self.getDenseLayer(fc1, self.weights['fc2'], self.biases['fc2'])\n",
    "        \n",
    "        ... 略 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu5e4b63iQlR"
   },
   "source": [
    "### Optimizer的選擇\n",
    "\n",
    "我們可以自由的替換我們想要使用的Optimizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Andz7YIiQlS"
   },
   "outputs": [],
   "source": [
    "# define training operation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "self.train_op = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc9mLzG0iQlU"
   },
   "source": [
    "在Tensorflow中目前有以下十種Optimizer供我們使用。\n",
    "\n",
    "* tf.train.GradientDescentOptimizer\n",
    "* tf.train.AdadeltaOptimizer\n",
    "* tf.train.AdagradOptimizer\n",
    "* tf.train.AdagradDAOptimizer\n",
    "* tf.train.MomentumOptimizer\n",
    "* tf.train.AdamOptimizer\n",
    "* tf.train.FtrlOptimizer\n",
    "* tf.train.ProximalGradientDescentOptimizer\n",
    "* tf.train.ProximalAdagradOptimizer\n",
    "* tf.train.RMSPropOptimizer\n",
    "\n",
    "如果想要了解每個Optimizer的演算法可以參考[這篇有詳細的說明](http://ruder.io/optimizing-gradient-descent/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffphVTiniQlV"
   },
   "source": [
    "### 來看看程式怎麼寫\n",
    "\n",
    "講了那麼多，來看看完整的程式怎麼寫？照慣例，先畫個流程圖。\n",
    "\n",
    "![DNNLogisticClassification](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.003.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiGskfdpiQlW"
   },
   "outputs": [],
   "source": [
    "class DNNLogisticClassification:\n",
    "\n",
    "    def __init__(self, n_features, n_labels,\n",
    "                 learning_rate=0.5, n_hidden=1000, activation=tf.nn.relu,\n",
    "                 dropout_ratio=0.5, alpha=0.0):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "        self.graph = tf.Graph()  # initialize new graph\n",
    "        self.build(learning_rate, n_hidden, activation,\n",
    "                   dropout_ratio, alpha)  # building graph\n",
    "        self.sess = tf.Session(graph=self.graph)  # create session by the graph\n",
    "\n",
    "    def build(self, learning_rate, n_hidden, activation, dropout_ratio, alpha):\n",
    "        # Building Graph\n",
    "        with self.graph.as_default():\n",
    "            ### Input\n",
    "            self.train_features = tf.placeholder(tf.float32, shape=(None, self.n_features))\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=(None, self.n_labels))\n",
    "\n",
    "            ### Optimalization\n",
    "            # build neurel network structure and get their predictions and loss\n",
    "            self.y_, self.original_loss = self.structure(features=self.train_features,\n",
    "                                                         labels=self.train_labels,\n",
    "                                                         n_hidden=n_hidden,\n",
    "                                                         activation=activation,\n",
    "                                                         dropout_ratio=dropout_ratio,\n",
    "                                                         train=True)\n",
    "            # regularization loss\n",
    "            self.regularization = \\\n",
    "                tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights.values()]) \\\n",
    "                / tf.reduce_sum([tf.size(w, out_type=tf.float32) for w in self.weights.values()])\n",
    "\n",
    "            # total loss\n",
    "            self.loss = self.original_loss + alpha * self.regularization\n",
    "\n",
    "            # define training operation\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "            ### Prediction\n",
    "            self.new_features = tf.placeholder(tf.float32, shape=(None, self.n_features))\n",
    "            self.new_labels = tf.placeholder(tf.int32, shape=(None, self.n_labels))\n",
    "            self.new_y_, self.new_original_loss = self.structure(features=self.new_features,\n",
    "                                                                 labels=self.new_labels,\n",
    "                                                                 n_hidden=n_hidden,\n",
    "                                                                 activation=activation)\n",
    "            self.new_loss = self.new_original_loss + alpha * self.regularization\n",
    "\n",
    "            ### Initialization\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def structure(self, features, labels, n_hidden, activation, dropout_ratio=0, train=False):\n",
    "        # build neurel network structure and return their predictions and loss\n",
    "        ### Variable\n",
    "        if (not self.weights) or (not self.biases):\n",
    "            self.weights = {\n",
    "                'fc1': tf.Variable(tf.truncated_normal(shape=(self.n_features, n_hidden))),\n",
    "                'fc2': tf.Variable(tf.truncated_normal(shape=(n_hidden, self.n_labels))),\n",
    "            }\n",
    "            self.biases = {\n",
    "                'fc1': tf.Variable(tf.zeros(shape=(n_hidden))),\n",
    "                'fc2': tf.Variable(tf.zeros(shape=(self.n_labels))),\n",
    "            }\n",
    "        ### Structure\n",
    "        # layer 1\n",
    "        fc1 = self.get_dense_layer(features, self.weights['fc1'],\n",
    "                                   self.biases['fc1'], activation=activation)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1, keep_prob=1-dropout_ratio)\n",
    "\n",
    "        # layer 2\n",
    "        logits = self.get_dense_layer(fc1, self.weights['fc2'], self.biases['fc2'])\n",
    "\n",
    "        y_ = tf.nn.softmax(logits)\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "                 tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "        return (y_, loss)\n",
    "\n",
    "    def get_dense_layer(self, input_layer, weight, bias, activation=None):\n",
    "        # fully connected layer\n",
    "        x = tf.add(tf.matmul(input_layer, weight), bias)\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X, y, epochs=10, validation_data=None, test_data=None, batch_size=None):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        random.seed(9000)\n",
    "        if not batch_size:\n",
    "            batch_size = N\n",
    "\n",
    "        self.sess.run(self.init_op)\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %2d/%2d: ' % (epoch+1, epochs))\n",
    "\n",
    "            # mini-batch gradient descent\n",
    "            index = [i for i in range(N)]\n",
    "            random.shuffle(index)\n",
    "            while len(index) > 0:\n",
    "                index_size = len(index)\n",
    "                batch_index = [index.pop() for _ in range(min(batch_size, index_size))]\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.train_features: X[batch_index, :],\n",
    "                    self.train_labels: y[batch_index],\n",
    "                }\n",
    "                _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "\n",
    "                print('[%d/%d] loss = %9.4f     ' % (N-len(index), N, loss), end='\\r')\n",
    "\n",
    "            # evaluate at the end of this epoch\n",
    "            y_ = self.predict(X)\n",
    "            train_loss = self.evaluate(X, y)\n",
    "            train_acc = self.accuracy(y_, y)\n",
    "            msg = '[%d/%d] loss = %8.4f, acc = %3.2f%%' % (N, N, train_loss, train_acc*100)\n",
    "\n",
    "            if validation_data:\n",
    "                val_loss = self.evaluate(validation_data[0], validation_data[1])\n",
    "                val_acc = self.accuracy(self.predict(validation_data[0]), validation_data[1])\n",
    "                msg += ', val_loss = %8.4f, val_acc = %3.2f%%' % (val_loss, val_acc*100)\n",
    "\n",
    "            print(msg)\n",
    "\n",
    "        if test_data:\n",
    "            test_acc = self.accuracy(self.predict(test_data[0]), test_data[1])\n",
    "            print('test_acc = %3.2f%%' % (test_acc*100))\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        return (np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/predictions.shape[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._check_array(X)\n",
    "        return self.sess.run(self.new_y_, feed_dict={self.new_features: X})\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "        return self.sess.run(self.new_loss, feed_dict={self.new_features: X,\n",
    "                                                       self.new_labels: y})\n",
    "\n",
    "    def _check_array(self, ndarray):\n",
    "        ndarray = np.array(ndarray)\n",
    "        if len(ndarray.shape) == 1:\n",
    "            ndarray = np.reshape(ndarray, (1, ndarray.shape[0]))\n",
    "        return ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "WhA796JIiQlY",
    "outputId": "bd3a3c6c-4a4d-4fd2-8fe6-b19b9881b6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "train_data = mnist.train\n",
    "valid_data = mnist.validation\n",
    "test_data = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "JqLadkHSiQlb",
    "outputId": "b631c594-ebd7-449b-a5d6-58b916bacacc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/ 3: \n",
      "[55000/55000] loss =   0.4948, acc = 88.12%, val_loss =   0.5729, val_acc = 88.14%\n",
      "Epoch  2/ 3: \n",
      "[55000/55000] loss =   0.3343, acc = 91.21%, val_loss =   0.3831, val_acc = 91.04%\n",
      "Epoch  3/ 3: \n",
      "[55000/55000] loss =   0.2890, acc = 92.73%, val_loss =   0.3708, val_acc = 92.06%\n",
      "test_acc = 91.17%\n"
     ]
    }
   ],
   "source": [
    "model = DNNLogisticClassification(\n",
    "    n_features=28*28,\n",
    "    n_labels=10,\n",
    "    learning_rate=0.5,\n",
    "    n_hidden=1000,\n",
    "    activation=tf.nn.relu,\n",
    "    dropout_ratio=0.5,\n",
    "    alpha=0.01,\n",
    ")\n",
    "model.fit(\n",
    "    X=train_data.images,\n",
    "    y=train_data.labels,\n",
    "    epochs=3,\n",
    "    validation_data=(valid_data.images, valid_data.labels),\n",
    "    test_data=(test_data.images, test_data.labels),\n",
    "    batch_size = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uB1VWP_iQld"
   },
   "source": [
    "跟上次的結果比，你會發現有長足的進步，精確率來到90幾，大家可以[下載程式碼](https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py)，試著調整參數使得DNN Model的精確率可以更高，參數包含：\n",
    "* Hidden Layer的神經元數量\n",
    "* 不同的Activation Function\n",
    "* 不同的Batch Size\n",
    "* 調整Weight Regularization的比例\n",
    "* 調整Dropout Ratio\n",
    "* 選擇不同Optimizer\n",
    "* 使得DNN更深\n",
    "\n",
    "調整Model是重要的工作，試著自己動手做做看，你可以讓你的Model有多準呢？"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_Build_First_DNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
